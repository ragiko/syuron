\chapter{文書の比較方法}
本章では文書から特徴量を計算し，文書検索に応用する方法について述べる．
まず初めに，テキストマイニングの分野で広く用いられている発見的手法のTF-IDFによる文書検索手法と，
TF-IDFによる文書検索手法を改良したBM25について説明する．
次に確率的言語モデルであるクエリ尤度モデルを用いた文書検索手法について述べる．
さらに，他の技術を用いた文書検索手法を提案する．
\section{TF-IDFを用いた文書検索}
本節では，クエリのTF-IDFベクトルと文書のTF-IDFベクトルを比較する手法について述べる．
\subsection{ユークリッド距離}
% TODO:ユークリッド距離の図，欠点が分かるように
ユークリッド距離は，ベクトル空間上の2ベクトルの距離である．
ベクトル空間上の2ベクトル$X = {x_1, x_2, ..., x_d}, Y = {y_1, y_2, ..., y_d}$が存在したとき，
ユークリッド距離$Euc(X, Y)$は式(\ref{eq_euc})で計算される．
\begin{equation}
    Euc(X, Y) = \sqrt{\sum^{d}_{i=1}{x_i^2 - y_i^2}}    \label{eq_euc}
\end{equation}
ユークリッド距離は，ベクトルの角度だけでなく，ベクトルの大きさによっても変わる．
すなわちTF-IDFのような文書ベクトルを比較する際には，文書長によっても類似度が変化する．
そのため文書検索においては，1クエリに対して長さが異なる2つ以上の文書を比較すると，正しく類似度を計算することができない問題がある．

\subsection{コサイン類似度} \label{sec_cosine}
% TODO:コサイン類似度の図
コサイン類似度は，ユークリッド距離と同じく，2つのベクトルの類似度を計る手法の一つである．
コサイン類似度は，2つのベクトル$X = {x_1, x_2, ..., x_d}, Y = {y_1, y_2, ..., y_d}$が存在したとき，
$X, Y$の成す角度$\theta$が小さいほど類似度$cos(\theta)$が高いとする手法であり，式(\ref{eq_cos})で計算される．
\begin{equation}
    cos(\theta) = \frac{X \cdot Y}{|X||Y|}  \label{eq_cos}
\end{equation}
コサイン類似度はベクトルの長さに依存せず，角度のみで類似度を計算する．
そのため，TF-IDFのような文書ベクトルの比較の際に，それぞれのベクトルの大きさ，すなわち文書長に依存しないため，
ユークリッド距離よりも類似度比較に優れているとされる．

\subsection{Webクエリ拡張}   \label{sec_webquery}
2文書をTF-IDFベクトルで比較する場合，文書中に存在しない語について比較できない．
また本研究で扱う文書検索の問題において，一般にクエリは短く，少語彙であることが多い．
すなわち，クエリの持つ語が少ないために，比較がうまく行えない場合がある．
また入力が音声クエリの場合，音声認識誤りによって認識が正しく行われず，本来の意図とは違う意味のクエリになる可能性がある．
これらの問題に対処するために，クエリのTF-IDFベクトルをWeb文書を用いて補完する，Webクエリ拡張を行う．

Webクエリ拡張は，クエリに関連すると考えられる文書を用いて，クエリの語彙を補完する手法である．
まず原文クエリからTF-IDFを計算し，TF-IDF上位$n$件の単語を抽出することで，原文クエリのキーワードを得る．
次にこれらの語からYahooの検索エンジンに入力するためのWeb検索クエリを生成する．
Web検索クエリは，クエリから得られた上位$n$件の単語を用いてできる，全ての3単語の組み合わせを，スペース区切りにしたものである．
すなわち，Web検索クエリは$_n C _3$組できる．
また本研究では1検索クエリあたりの取得Webページ数は100とした．
Web検索にキーワード1単語を用いず，3単語で検索するのは，Web検索で得られる文書がよりクエリと類似する文書が得られると考えられるからである．
最後に得られたWeb文書全てを，1つの巨大な文書とみなし，TF-IDFのベクトルを計算する．
最後に，原文クエリのTF-IDFと，得られたWeb文書のTF-IDFのを重み付け線型結合することにより，Webクエリ拡張されたTF-IDFベクトルを得る．
Web文書には一般的に原文クエリよりも多くの語が含まれていると考えられる．そのため，Web文書から計算されたTF-IDFを原文クエリのTF-IDFに
加味することにより，原文クエリの少語彙性を克服することができると考えられる．

\section{BM25}
文章中の単語の出現回数を考えたとき，長い文章になるほど，同じ単語が複数回繰り返されることが多くなると考えられる．
その場合，文書長が長いほど，一部の単語のTF値が高くなるために，TF-IDFの値が過剰に高くなり，文書検索に悪影響を及ぼすことが考えられる．
その問題へ対策した手法として，BM25\cite{BM25}が提案されている．
BM25はTF-IDFと異なり，PRF(Probablistic Relevance Framework)に基づく文書検索手法であり，文書$D$と$N$個の単語から構成されるクエリ
$Q = {w_1, w_2, ...w_N}$に対して類似度$score(D, Q)$が計算される．
$score(D, Q)$はTF-IDFと文書長$|D|$，平均文書長$avg|D|$を用いて式(\ref{eq_bm25})で計算される．
\begin{eqnarray}
    score(d, q) = \sum_{w_i \in q} tf'(w_i, d) * idf(w_i) \label{eq_bm25}  \\
    tf'(w_i, d) = \frac{ tf(w_i) \cdot (k+1) }{ tf(w_i) + k(1-b + b \cdot \frac{ |d| }{ avg|d| })} \label{eq_bm25_tf}
\end{eqnarray}
ここで，$k$と$b$はパラメータであり，$1 \leq k \leq 2$である．
ここで式(\ref{eq_bm25_tf})に示す$tf'(w_i, d)$をTFの拡張とみれば，
式(\ref{eq_bm25})はTF-IDFの拡張であると見ることができる．
式(\ref{eq_bm25})によれば，文書長$|D|$が長い文書ほど分母中の$\frac{ |D| }{ avg|D| }$が大きくなり，
TFに相当する部分の重みが小さくなると考えることができる．
これはすなわち，長い文書ほど一部のTFの値が高くなる問題に対処していることを意味する． 
\section{クエリ尤度モデルに基づく文書検索}
TF-IDFは「特定文書に頻出し，なおかつ他の文書には出現しにくい語は重要である」という仮定に基づく，発見的手法であるが，
本節では数理的に説明可能な枠組みである，クエリ尤度モデル\cite{query_likelihood}について述べる．

\subsection{クエリ尤度モデル}
情報検索は，クエリと検索対象の文書群が与えられた時に，クエリに適合する度合いにしたがって文書をランキングする問題と言い換えることができる．
すなわち，文書$D$がクエリ$Q$に適合する確率$P(D|Q)$を最大にする$D$を求める問題である．
これをベイズの定理を用いて変形すると，式(\ref{eq_query_likelihood1})となる．
\begin{equation}
    \argmax_{D} P(D|Q) = \argmax_{D} \frac{P(Q|D)P(D)}{P(Q)}    \label{eq_query_likelihood1}
\end{equation}
式(\ref{eq_query_likelihood1})において，$P(Q)$はクエリの生成確率分布であるが，文書$D$に依存しないため定数とみなすことができる．
また，$P(D)$は文書の生成確率分布であるが，どのような文書が与えられるかという事前知識が無い限り，一様であるとみなさざるを得ない．
よって式(\ref{eq_query_likelihood1})は式(\ref{eq_query_likelihood2})となる．
\begin{equation}
    \argmax_{D} P(D|Q) = \argmax_{D} P(Q|D) \label{eq_query_likelihood2}
\end{equation}
式(\ref{eq_query_likelihood2})の右辺は，文書に関する言語モデルがクエリを生成する確率$P(Q|D)$を表している．
これに基づく文書検索手法を，クエリ尤度モデルと呼ぶ．

クエリ尤度モデルにおいて文書を言語モデル化する際，ユニグラム言語モデルが頻繁に用いられる．
ユニグラム言語モデルは，語が周辺の語に依らず独立に生起するという仮定に基づいている．
ユニグラム言語モデルを$|\theta_D|$から$|Q|$個の語から成るクエリ$Q = \{q_1, q_2, ..., q_|Q|\}$が生成される尤度$P(Q|D)$は，式(\ref{eq_unigram})で計算される．
\begin{equation}
    P(Q|D) = \prod^{Q}_{l=1}P(q_l|\theta_d) \label{eq_unigram}
\end{equation}
ここで，クエリ$Q$中において，の語彙$V = \{w_1, w_2, ..., w_|V|\}$が複数回出現する可能性を考慮すると，式(\ref{eq_unigram})は式(\ref{eq_unigram2})と変形できる．
\begin{equation}
    P(Q|D) = \prod^{Q}_{w_i \in V}P(w_i|\theta_d)^{c(w_i, Q)} \label{eq_unigram2}
\end{equation}
ここで$c(w_i, Q)$はクエリ$Q$において語$w_i$が出現した回数である．
このように，クエリ尤度モデルによる文書検索問題は，$P(w_i|\theta_d)$の推定問題に帰着する．

本研究では，文書モデル$\theta_d$の推定方法に，文書中の語の相対頻度を用いた．
$P(w_i|\theta_d)$は式(\ref{eq_wordprob})で計算される．
\begin{equation}
    P(w_i|\theta_d) = \frac{c(w_i,D)}{|D|}    \label{eq_wordprob}
\end{equation}
ここで$c(w_i,D)$は文書$D$中の単語の出現回数を表し，$|D|$は文書$D$の単語数を表す．
これは文書における単語が観測された状況における，多項分布のパラメータの最尤推定に他ならない．

\subsection{ディリクレスムージング}  \label{sec_dirichlet}
式(\ref{eq_unigram})において，文書中に出現しない語，すなわち出現確率が0の語が1つでもクエリ中に存在すると，尤度が0となってしまう．
その場合，クエリ語が1つも存在しない文書と，クエリ語を一部含む文書との比較ができなくなる．これを零確率問題と呼ぶ．
また，文書長が短い場合には，式(\ref{eq_wordprob})では正しくモデルを推定することが出来ない．
これらの問題に対処するために，文書モデルのスムージングが行われる．
スムージングとは，何らかの方法で文書に含まれない語に微小の確率を割り振ることである．
これにより質問クエリに存在しない語を含む文書に対しては式(\ref{eq_unigram})に従ってそれらの微小な確率値が掛け合わされ，
結果としてクエリ尤度の値が小さくなり，前述の零確率問題に対処することができる．

本研究ではスムージングに，ディリクレスムージングを用いた．
ディリクレスムージングは，ディリクレ分布を文書多項分布の事前知識として導入するものであり，式(\ref{eq_dirichlet})によって計算される．
\begin{equation}
    P(w_i|\theta_d;\mu) = \frac{ c(w_i, D) + \mu P(w_i|\theta_C) }{ |D| + \mu } \label{eq_dirichlet}
\end{equation}
ここで，$\mu$はディリクレ事前分布のパラメータであり，正の値を持つ．また，$\theta_C$はディリクレスムージングのための文書コレクションから
推定された文書モデルである．
式(\ref{eq_dirichlet})を式(\ref{eq_wordprob})の拡張になるように変形すると，式(\ref{eq_dirichlet2})となる．
\begin{equation}
    P(w_i|\theta_d;\mu) = \frac{|D|}{|D|+\mu} \frac{c(w_i, D)}{|D|} + \frac{\mu}{|D|+\mu}P(w_i|\theta_C)  \label{eq_dirichlet2}
\end{equation}
式(\ref{eq_dirichlet2})によれば，補完の度合い$\frac{\mu}{|D|+\mu}$が文書長$|D|$によって補完の度合いが変化する．
これは長い文書ほど文書中の単語の観測サンプル数が大きくなるためにスムージングの必要性が低くなり，
逆に短い文書ほど必要性が高くなることを反映しており，前述の短い文書においてクエリ語が存在しない問題を解決している．

\subsection{Web文書を用いたディリクレスムージングの拡張} \label{sec_expanddirichlet}
本研究ではディリクレスムージングのための文書コレクションとして，検索対象の文書群を用いる．
しかし\ref{sec_webquery}節で述べたように，クエリに関連のある文書群を検索に用いることで，
クエリに対する言語モデルの表現力が向上し，より検索精度を向上させることができることが先行研究よりわかっている．
そこで本研究ではWeb文書を用いたディリクレスムージングの拡張を行う．
本手法では固定文書コレクション$\theta_C$と動的文書コレクション$\theta_W$の2つを用意し，それぞれを用いて文書モデルを作成し，スムージングに用いる．
ここでは固定文書コレクションに検索対象の文書群を，動的文書コレクションに\ref{sec_webquery}で述べた手法で収集されたWeb文書群を用いる．
式(\ref{eq_expanddirichlet})に本手法によってスムージングが行われたクエリ尤度モデルの式を示す．
\begin{flalign}
    & P(w_i|\theta_C; \mu; \nu) = \nonumber \\ 
    & \frac{|D|}{|D|+\mu+\nu}\frac{c(w_i, D)}{|D|} + \frac{\mu}{|D|+\mu+\nu}P(w_i|\theta_C) & \nonumber \\
    & + \frac{\nu}{|D|+\mu+\nu}P(w_i|\theta_W)  \label{eq_expanddirichlet}
\end{flalign}
ここで$P(w_i|\theta_W)$はWeb文書集合$W$内での語の生起確率であり，$\nu$はディリクレスムージングのパラメータであり，正の値を取る．

\subsection{LDAを用いたWeb文書の重み付けの拡張}
\ref{sec_expanddirichlet}節で収集したWeb文書群には，重要な情報を含む文書もあれば，あまり有用な情報を持っていない文書も存在すると考えられる．
そこで，より重要な情報を持つWeb文書に対しては，スムージングの際により大きな重みを持たせることで検索精度が向上することがわかっている．
本研究では検索対象文書のトピックに近い文書ほど重要な情報を持っているとして大きな重みを持たる．
まずWeb文書と検索対象文書それぞれに対して，LDA(Latent Dirichlet Allocation)を用いてトピック混合比ベクトル
$\gamma = \{\gamma_1, \gamma_2, ..., \gamma_|Z| \}$を計算する．
ここでLDAとはトピックモデルの1つであり，文書中に$|Z|$個の潜在トピック$Z = (z_1, z_2, ..., z_|Z|)$が存在すると仮定した言語モデルである．
% 文書中の潜在トピック$Z = (z_1, z_2, ..., z_|Z|)$の生成確率$\theta = (\theta_1, \theta_2, ..., \theta_|Z|)$が

その後，Web文書$W_i \in W$と検索対象文書$D_m \in D$のトピックの類似度$\sigma(W, D_m)$を，全ての検索対象文書$D$に対してコサイン類似度を用いて計算する．
その後，類似度の平均$\sigma(W_i | D)$をそのWeb文書の重みとする．
\begin{equation}
    \sigma(W_i | D) = \frac{1}{|D|}\sum^{|D|}_{m=1}\sigma(W_i, D_m) \label{eq_webweight}
\end{equation}
最後に計算された重み$\sigma(W_i | D)$を用いて，Web文書における語$w_i$の生起確率$P(w_i|\theta_W)$を式(\ref{eq_webprob})で計算する．
\begin{equation}
    P(w_i|\theta_W) = \frac{ \sum^{|W|}{l=1}\sigma(W_l, D)c(w_i, W_l) }{ \sum^{w}_{i=1} \sum^{|W|}{l=1}\sigma(W_l, D)c(w_i, W_l)}   \label{eq_webprob}
\end{equation}
ここで$w = {w_1, w_2, ..., w_|w|}$はWeb文書集合$W$中に出現する総単語数である．

\section{単語の意味的情報を用いた文書検索手法}  \label{sec_word2vec}
% \begin{itemize}
%     \item Word2vecは単語の持つ意味をベクトル空間上に表現する．
%     \item Word2vecは「ある単語の周辺にある語はお互いに近い意味を持つ」という仮定をもとにしている．
%     \item Word2vecはContinuous bag-of-wordsまたはskip-gramにより，単語の意味を表現する．(これらの違いは何か？)
%     \item 単語の意味の足し算，引き算が可能である．(King - man + woman = queen)
%     \item 学習は，線形ロジスティック回帰で行う．(これはどういったロジックなのか？)
%     \item 学習は形態素に分かち書きされた平文テキストを入力とし，予め指定した次元数で単語をベクトル空間で表現するように行われる．
%     \item 次元数の最適値は，学習コーパスの述べ単語数に比例することが先行研究で知られている．
%     \item 本研究では，CSJ(Corpus of spontaneous Japanese, 日本語話し言葉コーパス)と，NTCIR11 SpokenQuery\&Docのコーパス中の語を
%           Web検索キーワードとして取得したWeb文書を，Word2vecの学習コーパスとして用いた．
% \end{itemize}
%
本節では単語の意味的情報を考慮して文書ベクトルを生成し，比較する手法について述べる．
本手法ではまず，文書中の語に対してTF-IDFを計算し，値が上位の単語5件を，文書のキーワードとして取り出す．
次に抽出した単語5個それぞれに対してword2vecを用いて意味的情報を考慮したベクトルを割り当てる．
その後，ベクトルを平均することにより，文書の意味的情報を考慮したベクトルを生成する．
クエリと検索対象文書に対して，文書ベクトルを計算し，\ref{sec_cosine}節で述べたコサイン類似度で比較することによって文書検索を行う．

% まず，文書中の単語全てに対してWord2vecを用いてベクトルを割り当てる．
% 次に単語の位置情報を保存するために，近辺2単語のベクトルを用いて畳み込みを行う．
% その後，畳み込みが行われたベクトル群の各次元の最大値を取り出

% 本研究では文書中の単語から文書ベクトルを計算する手法として，Convolutional way\cite{convolutionalway}を用いる．
% Convolutional wayは，何らかの方法で単語に単語ベクトルを割り当て，その単語ベクトルから文書ベクトルを生成する方法である．
