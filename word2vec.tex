\chapter{単語のベクトル表現}
本章では単語をベクトルで表現する技術について解説する．
まず最も簡単な1-of-K表現について述べ，次に2単語のベクトル間距離が近いほど，意味も近いとする，単語の分散表現について述べる．
その後，単語の分散表現を学習するために用いるニューラルネットワークについて説明し，
実際に単語の分散表現を学習するツールであるword2vecのアルゴリズムについて解説する．

\section{1-of-K表現}
1-of-K表現とは，語彙が$K$単語存在するとき，単語をK次元かつ長さが1である，2値ベクトルで表現したものである．
このベクトル表現では，異なる任意の2単語のユークリッド距離が全て等しいという特徴を持つ．
単語間の距離を意味の近さと捉えたとき，この表現方法は現実的ではないという問題がある．
\section{単語の分散表現}
単語の分散表現とは，1-of-Kベクトルの情報を，もっと低い次元の実数ベクトルで表現したものである．
単語の分散表現を作る方法はいくつか存在するが，
後述するword2vecでは，ニューラルネットワークを用いて1-of-K表現から単語の分散表現を学習する．
1-of-K表現では単語が全て等距離であったが，単語の分散表現では単語間の距離が単語の意味的な距離になることが期待できる．

\section{ニューラルネットワーク}
% 中間層，出力層
本節ではword2vecで用いられるニューラルネットワークの基礎について説明する．
ニューラルネットワークは生物の神経細胞(ニューロン)を参考に作られたモデルであり，
$N$入力$M$出力(ただし，$N, M$は自然数)のベクトル写像である．
ニューラルネットワークは複数の形式ニューロンによって構成され，形式ニューロンの出力が別の形式ニューロンの入力になるように構成されている．
\subsection{形式ニューロン}
形式ニューロンは，$N$入力1出力のベクトル写像である．
形式ニューロンは$N$個の入力から1つの数値$y^{IN}$を計算し，それを形式ニューロンの持つ活性化関数$f$への入力とする．
形式ニューロンの出力$y^{OUT}$は，$y^{IN}$に活性化関数$f$へ適用したものとなる．
\begin{equation}
    y^{OUT} = f(y^{IN}) \label{eq_neuron}
\end{equation}
形式ニューロンには，$N$個の入力毎に決められた重み$w=(w_1, w_2, ..., w_N)$が与えられている．
形式ニューロンへの入力$x$が$x=(x_1, x_2, ..., x_N)$のとき，活性化関数への入力$y^{IN}$は式(\ref{eq_yin})で計算される．
\begin{equation}
    y^{IN} = wx^T = \sum^N_{i=1}w_ix_i \label{eq_yin} 
\end{equation}

% 活性化関数
出力層の活性化関数にはソフトマックス関数が一般的に用いられおり，word2vecにおいてもソフトマックスを簡易化したものが用いられている．
ソフトマックス関数は式(\ref{eq_softmax})で計算される．
\begin{equation}
    y_i^{OUT} = \frac{exp(y_i^{IN})}{\sum_k exp(y_k^{IN})}  \label{eq_softmax}
\end{equation}

\section{word2vec}
word2vec\cite{word2vec}とは，形態素に分かち書きされた文章から，ニューラルネットワークを用いて単語の分散表現を学習する手法およびツールである．
word2vecでは，文章中のある注目単語の前後$n$単語に出現する語は互いに関係があるという仮定に基づいて，単語の分散表現を学習する．
word2vecで単語の分散表現を学習する際には，CBOW(Continuous Bag-of-Words)とSkip-gramの，どちらかのニューラルネットワークを用いて学習する．
また本研究ではword2vecのモデルに，CSJ(Corpus of spontaneous Japanese, 日本語話し言葉コーパス)を用いて学習したモデルと，
NTCIR11 SpokenQuery\&Docの検索対象文書から，\ref{sec_webquery}に倣って取得したWeb文書から学習したモデルの2つを用いている．
\subsection{CBOW}
CBOWでは，注目単語の前後$n$単語のBag-of-Wordsを入力とし，注目単語を出力するニューラルネットワークを学習する．
ここでBag-of-Wordsとは，単語の1-of-K表現を足し合わせたものを表すものとする．
CBOWでは計算の高速化のために，中間層の活性化関数には恒等写像を用いている．
また，入力層から隠れ層への入力の重み$w$が全て均一の値になっている．
\subsection{Skip-gram}
Skip-gramでは，ある注目単語1つの1-of-Kが与えられた時，その前後$n$単語のBag-of-Wordsを出力とするニューラルネットワークを学習する．
