\chapter{文書検索のための自然言語処理}
本章では文書から特徴量を計算し，文書検索に応用する方法について述べる．
最初に，文章を単語列にする形態素解析について説明する．また，音声から文書を生成する音声認識について解説する．
次に，テキストマイニングの分野で広く用いられている発見的手法のTF-IDFによる文書検索手法について説明する．
そして，確率的言語モデルであるクエリ尤度モデルを用いた文書検索手法について述べる．
さらに，他の技術を用いた文書検索手法を提案する．
最後に，文書検索の評価尺度の1つであるMAP(Mean Average Precision)について説明する．

\section{形態素解析}
前処理として形態素解析を行う．
形態素とは，｢ある言語において，それ以上分解すると意味をなさなくなるところまで分解して抽出された，音素のまとまりの一つ一つ｣である．
日本語においては，形態素は単語と同じ意味を持つ．
形態素解析とは，与えられた文字列を形態素の列に分割し，品詞を解析する作業である．
本研究における形態素解析の目的は，単語の出現回数のカウントを可能にし，単語が名詞であるかどうかを判定することにある．
本研究では，形態素解析にMeCab\cite{MeCab}を用いた．

\subsection{MeCab}
MeCabとは， 京都大学情報学研究科-日本電信電話株式会社コミュニケーション科学基礎研究所 共同研究ユニットプロジェクトを通じて開発された
オープンソース 形態素解析エンジンである．
% 言語，辞書，コーパスに依存しない汎用的な設計を基本方針としている．
パラメータの推定にCRF(Conditional Random Fields)を用いており，隠れマルコフモデルを採用している形態素解析器のChaSen\cite{ChaSen}より
精度が向上している．
また，外部から容易に利用できるようなインターフェースを備えているため，他のシステムに組み込みやすいという特徴を持つ．

% 
% 田口追加
%

\section{音声認識}
現在の音声認識は，多くの場合確率モデルを用いた統計的手法に基づいている．従ってどのような統計的データを持つモデルを構築するかによって，音声認識結果は大きく変わることになる．音声$X$に対して，任意の単語列 $d$の中から最も適合する単語列 $d'$ を求める処理は，式(\ref{onsei_ninsiki1})として定式化される．\\

\begin{equation}
    d' = \argmax_{d} P(d|X) 
    \label{onsei_ninsiki1}
\end{equation}
ここでベイズの定理を適応すると，式(\ref{onsei_ninsiki1})は式(\ref{onsei_ninsiki2})へと変形することができる．

\begin{equation}
    d' = \argmax_{d}  \frac{P(X|d)P(d)}{P(X)} 
    \label{onsei_ninsiki2}
\end{equation}
この分母 $P(X)$ は $d$ の決定に影響しないため無視し，最終的に式(\ref{onsei_ninsiki3})のように変形される．

\begin{equation}
    d' \propto \argmax_{d}  P(X|d)P(d) 
    \label{onsei_ninsiki3}
\end{equation}
結果，音声認識は $P(X|d)$と$P(d)$によってモデル化されると考えることができる．$P(X|d)$は，単語列 $P(d)$に対して特徴ベクトル $X$ が観測される確率である．また，$P(d)$ は単語列$d$の言語的な生起確率である．

\subsection{音響モデル}
音響モデルとは，音素や音節の周波数パターンを用いて入力音声のマッチングを行うための確率的モデルである．式(\ref{onsei_ninsiki3})の $P(X|d)$ は単語列 $d = (w_1, w_2, ..., w_{|d|})$ に対する音響パターンが $X$ である確率を表し，音響モデルと照合することによって計算される．音響モデルはあらゆる文や単語パターンの音響確率を計算できるように音素に代表される音韻単位で用意される．すなわち単語列は音素列 $m_1, m_2, ... , m_I $ に展開され，式(\ref{onkyo_model1})で計算される．

\begin{equation}
    P(X|d) = \prod_{I}  P(X_i|m_i) 
    \label{onkyo_model1}
\end{equation}

ここで $P(X|m_i)$ とは音素単位の音響的特徴量を表現したモデルである．現在広く用いられているモデルは隠れマルコフモデル(HMM: Hidden Markov Model)である．HMMは定常な信号源を確率的に切り替えていくことで，時々刻々と変化する非定常信号を表現する．

\subsection{統計的言語モデル}

言語モデルとは，式(\ref{onsei_ninsiki3})の $P(d)$ にあたる，単語の生起確率を与えるモデルである．統計的なモデルに基づく言語モデルの場合，推定対象となる単語を前にある単語との接続情報から推定する．このとき文の先頭からの全ての単語の接続情報を考慮するのが理想であるが，現在では単語の直前 $N-1$ 単語の接続情報で近似する手法が一般的であり，この手法はN-gramと呼ばれる．以下にN-gramの説明を示す． 
 $N$ 個の単語の接続を確率として表すと，式(\ref{onkyo_model2})のように示される．

\begin{equation}
    P(w_i|w_{i-N+1}, ..., w_{i-1})) = \frac{ c(w_{i-N+1}, ..., w_{i-1}) }{ c(w_{i-N+1}, ..., w_{i-1}, w_i) }  
    \label{onkyo_model2}
\end{equation}

ここで $c(w_{i-N+1}, ..., w_{i-1})$ は単語列 $w_{i-N+1}, ..., w_{i-1}$ がこの順序で同時生起する頻度である．日本語の文章は単語の定義が明確でないため，予め形態素解析を行いひとつの単語を単位としてN-gramを構成することが一般的である． 
N-gramを構成する際の問題点として，「文法的には正しいが，偶然にも学習データのテキスト内に1回も出現しない」単語が出現した場合，N-gramが存在しないため確率を推定することができない．また，「大量に存在するテキスト内に，1回しか出現しない」単語のN-gramがほぼ0となり，そのような単語は大量に存在するので，計算コストが必要以上に増加してしまう．これらの問題の解決法として，頻出単語以外の単語を未知語として処理する，スムージングが挙げられる．スムージング\cite{ngram_smooth1}を行うことにより，計算時間を減らしつつN-gram構築時に一度も出現しなかった単語の組み合わせの生起確率を推定することができる．
% 参考: 未知語 http://www2.nict.go.jp/univ-com/multi_trans/member/mutiyama/corpmt/6.pdf

%
% 田口 END
%

% TODO: TFの説明とか追加しても良い

\section{TF-IDFを用いた文書検索} \label{sec_tfidf}
本節では，クエリのTF-IDFベクトルと文書のTF-IDFベクトルを比較する手法について述べる．
\subsection{ユークリッド距離}
% TODO:ユークリッド距離の図，欠点が分かるように
ユークリッド距離は，ベクトル空間上の2ベクトルの距離である．
ベクトル空間上の2ベクトル$X = \{x_1, x_2, ..., x_d\}, Y = \{y_1, y_2, ..., y_d\}$が存在したとき，
ユークリッド距離$Euc(X, Y)$は式(\ref{eq_euc})で計算される．
\begin{equation}
    Euc(X, Y) = \sqrt{\sum^{d}_{i=1}{(x_i - y_i)^2}}    \label{eq_euc}
\end{equation}
ユークリッド距離は，ベクトルの角度だけでなく，ベクトルの大きさによっても変わる．
すなわちTF-IDFのような文書ベクトルを比較する際には，文書長によっても類似度が変化する．
そのため文書検索においては，1クエリに対して長さが異なる2つ以上の文書を比較すると，正しく類似度を計算することができない問題がある．

\subsection{コサイン類似度} \label{sec_cosine}
% TODO:コサイン類似度の図
コサイン類似度は，ユークリッド距離と同じく，2つのベクトルの類似度を計る手法の一つである．
コサイン類似度は，2つのベクトル$X = \{x_1, x_2, ..., x_d\}, Y = \{y_1, y_2, ..., y_d\}$が存在したとき，
$X, Y$の成す角度$\theta$が小さいほど類似度$cos(\theta)$が高いとする手法であり，式(\ref{eq_cos})で計算される．
\begin{equation}
    cos(\theta) = \frac{X \cdot Y}{|X||Y|}  \label{eq_cos}
\end{equation}
コサイン類似度はベクトルの長さに依存せず，角度のみで類似度を計算する．
そのため，TF-IDFのような文書ベクトルの比較の際に，それぞれのベクトルの大きさ，すなわち文書長に依存しないため，
ユークリッド距離よりも類似度比較に優れているとされる．

\subsection{クエリ尤度モデル}
情報検索は，クエリと検索対象の文書群が与えられた時に，クエリに適合する度合いにしたがって文書をランキングする問題と言い換えることができる．
すなわち，文書$D$がクエリ$Q$に適合する確率$P(D|Q)$を最大にする$D$を求める問題である．
これをベイズの定理を用いて変形すると，式(\ref{eq_query_likelihood1})となる．
\begin{equation}
    \argmax_{D} P(D|Q) = \argmax_{D} \frac{P(Q|D)P(D)}{P(Q)}    \label{eq_query_likelihood1}
\end{equation}
式(\ref{eq_query_likelihood1})において，$P(Q)$はクエリの生成確率分布であるが，文書$D$に依存しないため定数とみなすことができる．
また，$P(D)$は文書の生成確率分布であるが，どのような文書が与えられるかという事前知識が無い限り，一様であるとみなさざるを得ない．
よって式(\ref{eq_query_likelihood1})は式(\ref{eq_query_likelihood2})となる．
\begin{equation}
    \argmax_{D} P(D|Q) = \argmax_{D} P(Q|D) \label{eq_query_likelihood2}
\end{equation}
式(\ref{eq_query_likelihood2})の右辺は，文書に関する言語モデルがクエリを生成する確率$P(Q|D)$を表している．
これに基づく文書検索手法を，クエリ尤度モデルと呼ぶ．

クエリ尤度モデルにおいて文書を言語モデル化する際，ユニグラム言語モデルが頻繁に用いられる．
ユニグラム言語モデルは，語が周辺の語に依らず独立に生起するという仮定に基づいている．
ユニグラム言語モデルを$|\theta_D|$から$|Q|$個の語から成るクエリ$Q = \{q_1, q_2, ..., q_|Q|\}$が生成される尤度$P(Q|D)$は，式(\ref{eq_unigram})で計算される．
\begin{equation}
    P(Q|D) = \prod^{Q}_{l=1}P(q_l|\theta_d) \label{eq_unigram}
\end{equation}
ここで，クエリ$Q$中において，の語彙$V = \{w_1, w_2, ..., w_|V|\}$が複数回出現する可能性を考慮すると，式(\ref{eq_unigram})は式(\ref{eq_unigram2})と変形できる．
\begin{equation}
    P(Q|D) = \prod^{Q}_{w_i \in V}P(w_i|\theta_d)^{c(w_i, Q)} \label{eq_unigram2}
\end{equation}
ここで$c(w_i, Q)$はクエリ$Q$において語$w_i$が出現した回数である．
このように，クエリ尤度モデルによる文書検索問題は，$P(w_i|\theta_d)$の推定問題に帰着する．

本研究では，文書モデル$\theta_d$の推定方法に，文書中の語の相対頻度を用いた．
$P(w_i|\theta_d)$は式(\ref{eq_wordprob})で計算される．
\begin{equation}
    P(w_i|\theta_d) = \frac{c(w_i,D)}{|D|}    \label{eq_wordprob}
\end{equation}
ここで$c(w_i,D)$は文書$D$中の単語の出現回数を表し，$|D|$は文書$D$の単語数を表す．
これは文書における単語が観測された状況における，多項分布のパラメータの最尤推定に他ならない．

\subsection{ディリクレスムージング}  \label{sec_dirichlet}
式(\ref{eq_unigram})において，文書中に出現しない語，すなわち出現確率が0の語が1つでもクエリ中に存在すると，尤度が0となってしまう．
その場合，クエリ語が1つも存在しない文書と，クエリ語を一部含む文書との比較ができなくなる．これを零確率問題と呼ぶ．
また，文書長が短い場合には，式(\ref{eq_wordprob})では正しくモデルを推定することが出来ない．
これらの問題に対処するために，文書モデルのスムージングが行われる．
スムージングとは，何らかの方法で文書に含まれない語に微小の確率を割り振ることである．
これにより質問クエリに存在しない語を含む文書に対しては式(\ref{eq_unigram})に従ってそれらの微小な確率値が掛け合わされ，
結果としてクエリ尤度の値が小さくなり，前述の零確率問題に対処することができる．

本研究ではスムージングに，ディリクレスムージングを用いた．
ディリクレスムージングは，ディリクレ分布を文書多項分布の事前知識として導入するものであり，式(\ref{eq_dirichlet})によって計算される．
\begin{equation}
    P(w_i|\theta_d;\mu) = \frac{ c(w_i, D) + \mu P(w_i|\theta_C) }{ |D| + \mu } \label{eq_dirichlet}
\end{equation}
ここで，$\mu$はディリクレ事前分布のパラメータであり，正の値を持つ．また，$\theta_C$はディリクレスムージングのための文書コレクションから
推定された文書モデルである．
式(\ref{eq_dirichlet})を式(\ref{eq_wordprob})の拡張になるように変形すると，式(\ref{eq_dirichlet2})となる．
\begin{equation}
    P(w_i|\theta_d;\mu) = \frac{|D|}{|D|+\mu} \frac{c(w_i, D)}{|D|} + \frac{\mu}{|D|+\mu}P(w_i|\theta_C)  \label{eq_dirichlet2}
\end{equation}
式(\ref{eq_dirichlet2})によれば，補完の度合い$\frac{\mu}{|D|+\mu}$が文書長$|D|$によって補完の度合いが変化する．
これは長い文書ほど文書中の単語の観測サンプル数が大きくなるためにスムージングの必要性が低くなり，
逆に短い文書ほど必要性が高くなることを反映しており，前述の短い文書においてクエリ語が存在しない問題を解決している．

\section{MAP} \label{sec_map}
SpokenQuery\&Docの評価尺度にはMAP(Mean Average Precision)が用いられている．
これはクエリ毎の平均適合率AP値の平均である．
クエリ$q$に対するAP値$AP(q)$は式(\ref{eq_ap})で計算される．
\begin{equation}
    AP(q) = \frac{1}{|N|} \sum^N_{i=1} \frac{i}{rank(i)}    \label{eq_ap}
\end{equation}
ここで$N$はクエリ$q$に対する正解文書の数，$rank(i)$はクエリ$q$の$i$番目の正解文書が，文書検索によって付けられた順位である．
AP値，MAPは共に[0, 1]の値をとる．
