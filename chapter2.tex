\chapter{文書検索のための自然言語処理}
本章では文書検索の観点から必要な，自然言語処理の基礎技術について述べる．
まず最初に，文章を単語列にする形態素解析について説明する．
次に自然言語処理において，情報検索に多く使われている特徴量の，TF-IDFについて述べる．
その後，単語の共起頻度を表すPMI(Pointwise Mutual Information)について述べる．
最後に，文書検索の評価尺度の1つであるMAP(Mean Average Precision)について説明する．

\section{形態素解析}
前処理として形態素解析を行う．
形態素とは，｢ある言語において，それ以上分解すると意味をなさなくなるところまで分解して抽出された，音素のまとまりの一つ一つ｣である．
日本語においては，形態素は単語と同じ意味を持つ．
形態素解析とは，与えられた文字列を形態素の列に分割し，品詞を解析する作業である．
本研究における形態素解析の目的は，単語の出現回数のカウントを可能にし，単語が名詞であるかどうかを判定することにある．
本研究では，形態素解析にMeCab\cite{MeCab}を用いた．

\subsection{MeCab}
MeCabとは， 京都大学情報学研究科-日本電信電話株式会社コミュニケーション科学基礎研究所 共同研究ユニットプロジェクトを通じて開発された
オープンソース 形態素解析エンジンである．
% 言語，辞書，コーパスに依存しない汎用的な設計を基本方針としている．
パラメータの推定にCRF(Conditional Random Fields)を用いており，隠れマルコフモデルを採用している形態素解析器のChaSen\cite{ChaSen}より
精度が向上している．
また，外部から容易に利用できるようなインターフェースを備えているため，他のシステムに組み込みやすいという特徴を持つ．

% 
% 田口追加
%

\section{音声認識}
現在の音声認識は，多くの場合確率モデルを用いた統計的手法に基づいている．従ってどのような統計的データを持つモデルを構築するかによって，音声認識結果は大きく変わることになる．音声$X$に対して，任意の単語列 $d$の中から最も適合する単語列 $d'$ を求める処理は，式(\ref{onsei1})として定式化される．\\

\begin{equation}
    d' = \argmax_{d} P(d|X) 
    \label{onsei_ninsiki1}
\end{equation}

% TODO: 式
ここでベイズの定理を適応すると，式(2.1)は式(2.2)へと変形することができる．

\begin{equation}
    d' = \argmax_{d}  \frac{P(X|d)P(d)}{P(X)} 
    \label{onsei_ninsiki2}
\end{equation}

% TODO: 式
また，$d$の最大値は分母の値に関わらないため，最終的に式(2.3)のように変形される．

\begin{equation}
    d' = \argmax_{d}  P(X|d)P(d) 
    \label{onsei_ninsiki3}
\end{equation}

結果，音声認識は $P(X|d)$と$P(d)$によってモデル化されると考えることができる．$P(X|d)$は，単語列 $P(d)$に対して特徴ベクトル $X$ が観測される確率である．また，$P(d)$ は単語列$d$の言語的な生起確率である．

\subsection{音響モデル}
音響モデルとは，音素や音節の周波数パターンを用いて入力音声のマッチングを行うための確率的モデルである．式(2.3)の $P(X|d)$ は単語列 $d = (w_1, w_2, ..., w_{|d|})$ に対する音響パターンが $X$ である確率を表し，音響モデルと照合することによって計算される．音響モデルはあらゆる文や単語パターンの音響確率を計算できるように音素に代表される音韻単位で用意される．すなわち単語列は音素列 $m_1, m_2, ... , m_I $ に展開され，式(2.4)で計算される．

\begin{equation}
    P(X|d) = \prod_{I}  P(X|m_i) 
    \label{onkyo_model1}
\end{equation}

ここで $P(X|m_i)$ とは音素単位の音響的特徴量を表現したモデルである．また，$x$ は入力音声の一部 $x \in X$ である．
現在広く用いられているモデルは隠れマルコフモデル(HMM: Hidden Markov Model)である．HMMは定常な信号源を確率的に切り替えていくことで，時々刻々と変化する非定常信号を表現する．

\subsection{統計的言語モデル}

言語モデルとは，式(2.3)の $P(d)$ にあたる，単語の生起確率を与えるモデルである．統計的なモデルに基づく言語モデルの場合，推定対象となる単語を前にある単語との接続情報から推定する．このとき文の先頭からの全ての単語の接続情報を考慮するのが理想であるが，現在では単語の直前 $N-1$ 単語の接続情報で近似する手法が一般的であり，この手法はN-gramと呼ばれる．以下にN-gramの説明を示す． 
 $N$ 個の単語の接続を確率として表すと，式(2.5)のように示される．

\begin{equation}
    P(w_i|w_{i-N+1}, ..., w_{i-1})) = \frac{ c(w_{i-N+1}, ..., w_{i-1}) }{ c(w_{i-N+1}, ..., w_{i-1}, w_i) }  
    \label{onkyo_model1}
\end{equation}

ここで $c(w_{i-N+1}, ..., w_{i-1})$ は単語列 $w_{i-N+1}, ..., w_{i-1}$ が同時生起する頻度である．日本語の文章は単語の定義が明確でないため，予め形態素解析を行いひとつの単語を単位としてN-gramを構成することが一般的である． 
N-gramを構成する際の問題点として，「文法的には正しいが，偶然にもテキスト内に1回も出現しない」単語が出現した場合，N-gramが存在しないため確率を推定することができない．また，「大量に存在するテキスト内に，1回しか出現しない」単語のN-gramがほぼ0となり，そのような単語は大量に存在するので，計算コストが必要以上に増加してしまう．これらの問題の解決法として，頻出単語以外の単語を未知語として処理する，スムージングが挙げられる．スムージングを行うことにより，計算時間を減らしつつN-gram構築時に一度も出現しなかった単語の組み合わせの生起確率を推定することができる．


%
%
%

\section{TF-IDF}
ここでは，現在の情報検索で最も多く使われているTF-IDFについて説明する．
TF-IDFとは，特定文書に頻出し，なおかつ他の文書には出現しにくい語を重要とする手法である．
文書$d$における単語$w$のTF-IDFは，TF(Term Frequency)とIDF(Inverse Document Frequency)を用いて式(\ref{eq_tfidf})で定義される．

\begin{equation}
    tfidf(w,d) = tf(w,d) * idf(w)   \label{eq_tfidf}
\end{equation}

TFは文書$d$における単語の出現回数である．
TFは，文書中に何度も繰り返し言及される語は重要であるという仮定に基づいている．
しかし「私」のような，あまりに頻度が高く，どの文書にも出現する語(汎用語)は，文書を特徴付ける上で重要ではないと言える．
この問題の対処のために，IDFを導入する．
IDFは$N$個の文書の集合$D$の中で，単語$w$の出現する文書が少ないほど高い値を取るものである．
汎用語はTFが高くなるが，どの文書にも出現するためにIDFが低くなるため，TF-IDFでは重要語として扱われない．
IDFの計算方法は様々なものがあるが，例えば式(\ref{eq_idf})で計算する．

\begin{equation}
    idf(w) = {\rm log} (1 + \frac{N}{df(w)})  \label{eq_idf}
\end{equation}

ここで$df(w)$は，文書集合$D$の中で単語$w$の出現する文書の数を表す．\\

\section{MAP}
SpokenQuery\&Docの評価尺度にはMAP(Mean Average Precision)が用いられている．
これはクエリ毎の平均適合率AP値の平均である．
クエリ$q$に対するAP値$AP(q)$は式(\ref{eq_ap})で計算される．
\begin{equation}
    AP(q) = \frac{1}{|N|} \sum^N_{i=1} \frac{i}{rank(i)}    \label{eq_ap}
\end{equation}
ここで$N$はクエリ$q$に対する正解文書の数，$rank(i)$はクエリ$q$の$i$番目の正解文書が，文書検索によって付けられた順位である．
AP値，MAPは共に[0, 1]の値をとる．
